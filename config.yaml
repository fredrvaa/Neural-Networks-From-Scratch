globals:
  loss_function: CrossEntropy # {MSE, CrossEntropy}
  learning_rate: 0.01
  batch_size: 1
  wreg: 0.00001
  wrt: L1 # {L1, L2, none}
layers:
  input:
    size: 100
  hidden:
#    - size: 5
#      activation: Relu # {Linear, Relu, Sigmoid, Tanh}
#      init_scheme: 'glorot_uniform' # 'uniform', 'glorot_uniform', 'glorot_normal'
#      weight_range: (-1, 1)
#      bias_range: (0, 0)
#      learning_rate: 0.1
    - size: 100
      activation: Relu # {Linear, Relu, Sigmoid, Tanh}
      init_scheme: 'uniform'
      weight_range: (-0.5, 0.5)
      bias_range: (0, 0)
    - size: 100
      activation: Relu # {Linear, Relu, Sigmoid, Tanh}
      init_scheme: 'uniform'
      weight_range: (-0.5, 0.5)
      bias_range: (0, 0)
    - size: 5
      activation: Relu # {Linear, Relu, Sigmoid, Tanh}
      init_scheme: 'uniform'
      weight_range: (-0.5, 0.5)
      bias_range: (0, 0)
  output:
    type: SoftmaxLayer # {SoftmaxLayer}
