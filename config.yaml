globals:
  loss_function: CrossEntropy # {MSE, CrossEntropy}
  learning_rate: 0.001
  batch_size: 10
  wreg: 0.001
  wrt: L2 # {L1, L2, none}
layers:
  input:
    size: 2500
  hidden:
#    - size: 5
#      activation: Relu # {Linear, Relu, Sigmoid, Tanh}
#      init_scheme: 'glorot_uniform' # 'uniform', 'glorot_uniform', 'glorot_normal'
#      weight_range: (-1, 1)
#      bias_range: (0, 0)
#      learning_rate: 0.1
    - size: 100
      activation: Relu # {Linear, Relu, Sigmoid, Tanh}
      init_scheme: 'uniform'
      weight_range: (-0.5, 0.5)
      bias_range: (0, 0)
    - size: 20
      activation: Relu # {Linear, Relu, Sigmoid, Tanh}
      init_scheme: 'glorot_uniform'
      weight_range: (-0.5, 0.5)
      bias_range: (0, 0)
    - size: 5
      activation: Relu # {Linear, Relu, Sigmoid, Tanh}
      init_scheme: 'glorot_uniform'
      weight_range: (-0.5, 0.5)
      bias_range: (0, 0)
  output:
    type: SoftmaxLayer # {SoftmaxLayer}
