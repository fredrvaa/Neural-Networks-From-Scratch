globals:
  loss_function: CrossEntropy # {MSE, CrossEntropy}
  learning_rate: 0.01
  batch_size: 32
  wreg: 0.001
  wrt: L1 # {L1, L2, none}
layers:
  input:
    size: 2500
  hidden:
    - size: 200
      activation: Relu # {Linear, Relu, Sigmoid, Tanh}
      weight_range: (-1, 1)
      bias_range: (0,0)
      learning_rate: 0.001
    - size: 20
      activation: Relu # {Linear, Relu, Sigmoid, Tanh}
      weight_range: (-1, 1)
      bias_range: (0,0)
      learning_rate: 0.001
    - size: 5
      activation: Relu # {Linear, Relu, Sigmoid, Tanh}
      weight_range: (-1, 1)
      bias_range: (0,0)
      learning_rate: 0.001
  output:
    type: SoftmaxLayer # {SoftmaxLayer}
