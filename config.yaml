globals:
  loss_function: CrossEntropy # {MSE, CrossEntropy}
  learning_rate: 0.1
  batch_size: 32
  wreg: 0.001
  wrt: L1 # {L1, L2, none}
layers:
  input:
    size: 2500
  hidden:
    - size: 100
      activation: Relu # {Linear, Relu, Sigmoid, Tanh}
      weight_range: (-0.1, 0.1)
      bias_range: (0,1)
      learning_rate: 0.1
    - size: 50
      activation: Relu # {Linear, Relu, Sigmoid, Tanh}
      weight_range: (-0.1, 0.1)
      bias_range: (0,1)
      learning_rate: 0.1
    - size: 20
      activation: Relu # {Linear, Relu, Sigmoid, Tanh}
      weight_range: (-0.1, 0.1)
    - size: 5
      bias_range: (0,1)
      learning_rate: 0.1
  output:
    type: SoftmaxLayer # {SoftmaxLayer}
